---
title: "2: Simple linear regression"
author: "Paul Robinson"
description: "An introduction to statistical modelling and inference via regression on one outcome and one predictor"
output:
  html_document:
    toc: true
    toc_expand: true
    toc_depth: 5
date: "8/17/2024"
date-modified: "9/06/2024"
categories: [R, regression]
citation: 
  url: https://paulauvirage.github.io/posts/2024-08-19-simple-linear-regression/ 
image: weblog-simple-linear-regression.png
draft: false
bibliography: references.bib
filters:
  - line-highlight
---

To start, click on the right hand arrow in the "Document structure" box, below.

::: {.callout-note collapse="true"}
## Document structure

This tutorial assumes that you are working in R Studio.

The main text followed is [@Gelman_Hill_Vehtari_2020], available for [free download](https://avehtari.github.io/ROS-Examples/index.html). Supplementary recommended books are [@Fox_Weisberg_2018], [@Alexander_2023] available [free to read online](https://tellingstorieswithdata.com/).

As with all posts in this group, the main text is interspersed with two types of blocks.

**Call-outs** of supporting information, such as this one, are folded away to avoid their interfering with a first read of the post. I recommend reading them all. You can unfold them by clicking on the far right arrow inside the box.

**Code blocks** are folded away with an arrow to the left of the numbered code block, for example `Code 2.1 Import packages to session`. They allow you to implement all of the analysis in the post on your computer. Each code block has numbered notes to the right, referring to particular lines. Click on the number and a box will pop up with a brief explanation of the code. Click again and the pop up closes. I am assuming you read the posts in sequence, so the earlier posts will have pop up explanations that are not repeated in the later code blocks. You can cut and paste the code in each code block into your own R scripting window in R Studio, by clicking on the clipboard in the right end of the block, but I recommend your typing it in, to get practice in how R code works in R Studio.
:::

## Working example - trophy hunting of lions


Trophy hunting of lions, particularly males, is a controversial, but [active](https://www.discountafricanhunts.com/hunts/hunt-lion-in-tanzania.html#:~:text=Hunt%20Lion%20in%20Tanzania&text=Rifle%20Only%201%20x%201%20Starting%20at%20%2455%2C000!), means of conserving populations. Selective killing of older lions is thought to result in fewer knock-on deaths, as older animals are less likely to be breeding and the killing of a younger breeding male can result in further lion deaths, particularly through infanticide, as a new male replaces the killed male in the pride [@whitman2004], [@loveridge2023]. The amount of black on a lion's nose is the best remote indicator of age . This measure has been taken for lions of known age in a long term study at the Serengeti, Tanzania. The data is available for re-analysis in the R package `abd` and has been used for an example of linear regression in the online book [Statistics for ecologists](https://statistics4ecologists-v2.netlify.app/linreg) [@fieberg_2024].

::: {.callout-note collapse="true"}
## The advantages of open data

This worked example is possible because the data has been made easily, publicly accessible. It is not essential, though it is helpful, that it is accessible through an `R` package. In a later post an optimal workflow for scientists will be discussed in which open data is a component.
:::

## The objectives of regression

Let's review what regression aims to do, applied to the lion nose data, in the call-out box below.

::: {.callout-note collapse="true"}
## The objectives of regression

Regression is the most widely used method of **statistical inference**, generalising from the sample on which measurements have been taken to the population for which a hypothesis or, more often and more usefully, the size and direction of effects, is sought.

The sample data for a regression problem, gathered for the project or reused, contains, for each unit on which measurements have been taken, the **outcome** it is hope to explain and one or more **predictors** which it is hoped can explain variability in the outcome with sufficient precision to be useful.

Regression, and statistical inference more generally, is therefore a problem of prediction in the face of uncertainty. There is uncertainty about whether:

-   the sample represents the population;
-   the measurements are truly measuring the features of interest ("measurement validity"); and
-   the mathematical relation between outcome and predictor is causal.

Regression usually seeks to predict the average outcome for the predictor(s). To be more explicit in applying this reasoning to the lion nose data set, the predictor is the amount of black on the nose and the outcome which it is aimed to predict is the age of a lion for which the predictor (nose black marks) has been measured, but the outcome (age) is not known.

An hypothesis test could establish whether or not there is a relation between measured nose markings and age, usually phrased as a **null hypothesis**; H~0~ there is no relation, versus the **alternative hypothesis** that H~0~ is not supported.

This course will not concern itself much with hypothesis testing, but rather the more useful measurement of effects; is there a predictable change in the amount nose marking with age?

It is very unlikely that every lion of a certain age will have exactly the same amount of nose markings. If this was true, you would not need to use statistics, but rather simple algebra. Variability, arising from whatever processes cause nose markings and/or measurement error, are likely to give a range of values for lions of the same age. A regression model uses this variability and the size of the sample data (number of individuals with both nose marking and age measured) to estimate and report on the uncertainty in any prediction. Depending on the method of inference used - Frequentist or Bayesian - the uncertainty is reported as **confidence intervals** or **credible intervals** which have subtly different meanings. This will be discussed more later. What is important to remember is that statistical modelling, as opposed to deterministic modelling, is: necessary when there is variability in the data; and is reported with measures of uncertainty. *add important uses of regression from Gelman p5?*
:::

## Analysis of the lion nose data with R


```{mermaid}
flowchart LR
  A[Question] --> B(Data)
  B --> C(Tidy)
  C --> D(Exploratory analysis)
  D --> E{Model}
  E --> F[Interpret]
  E --> G[Report]
```


::: {.callout-note collapse="true"}
## A recap of the ideal sequence of a statistical investigation

We will follow [@Cox_Donnelly_2011] in defining seven steps for an applied statistical investigation that integrates the subject matter (lion nose markings and age) and statistical techniques.

1.  Formulate and clarify a research question of subject-matter importance

-   Don't waste your time on questions for which few people are interested in the answer; make the question(s) clear.

2.  Design an investigation to produce secure answers.

-   bbb

3.  Produce effective and reliable measurement procedures.

-   ccc

4.  Preliminary analysis

-   Often referred to as Exploratory Data Analysis (EDA). Use simple methods with visuals and tables to check data and inform on model choise

5.  Model formulation and inference

-   Develop and apply a model to give answers, with some assessment of thier uncertainty

6.  Present conclusions

-   eee

7.  Interpret in subject matter terms and in relation to the knowledge base of the field

-   fff
:::

### Step 1. Subject matter questions

When possible choose an important question that is likely to be soluble from the data. The Lion is classified as globally [Vulnerable](https://www.iucnredlist.org/species/15951/259030422). Trophy hunting is economically valuable. Therefore an investigation of how selective trophy hunting could minimise the impact on the lion population size is of importance. There is evidence that trophy hunting older lions could cause less impact on the population and that lion age can be measured unobtrusively from the amount of black marking on the nose. The broad questions are:

a.  Is there a difference in nose marking with age (hypothesis test)?
b.  Can the difference be modeled to provide a measure of how nose markings change with age (regression model); and
c.  Can the model be applied, for example to regulate permits for trophy hunting?

### Step 4. Exploratory data analysis (EDA), jumping Steps 2 (Study Design) and 3 (Measurement validity) as existing data is being used.

Let's start with question b. The data is already collected, so the usual **Step 2** of designing a study to collect data and **Step 3** of measurement validity are not necessary. However, you should read the papers that have been produced using the data, as how the data was gathered (both design and measurement) could affect how it can be interpreted. Maybe contact the authors if you have a query.

A first task, and so usually the first lines of `R` code, is to import into the analysis session, from your local library on your computer, the necessary R packages for the data handling, manipulation and analysis (`Code 3.1`) Next, import the data into your session (`Code 3.2`).

::: {.callout-note collapse="true"}
## Package management in R

If you are familiar with software that comes ready packaged with all of its capabilities, R may seem strange. It follows a different philosophy of bundling up one or a few related tasks into a package. When R is first downloaded onto your computer, it comes with 15 "base" packages, of the c21,000 packages available in the cloud on the [Comprehensive R Archive network](https://cran.r-project.org/web/packages/) and others available from other cloud storage locations.

Any analysis usually needs more than the base packages. This course will use an extra 20 or so. You can, with an internet connection, download and install any extra packages onto your computer, where they will be added to your package library (see the packages tab in R Studio's lower right window). As developers work on packages, the changed version needs to be updated on your computer. You will not be prompted to update. In R Studio the 'packages' then 'update' tab (bottom right window) is available for this. It will, with an internet connection, search for those of your installed packages that have updates and give you the option to install the updated versions. Most packages are less than 1MB in size and few more than 5MB, so I recommend you check and update packages as often as you have a stable internet connection. I have about 100 packages and update (there usually is something to update) daily, but less often is fine.

Once a package is on your computer, it is not yet available to use in your current analysis session. For this you need to import the package from your library, with a line of code using the `library` function; for example `library(marginaleffects)` to import the Marginal Effects package. Repeat this for each package. In writing a script of R code for any work, you usually start with lines to import the necessary packages, as without them the subsequent code depending on them will not run.
:::

[**R Code 2.1** Import packages from your library into the current analysis session.]{.aside}
---
code-annotations: select
---

```{r}
#| message: false
#| code-summary: "R Code 2.1"
#| code-block-bg: true
#| highlight-style: github
library(abd) # a biological data package that includes the lion nose data     # <1>
library(tidyverse) # eight packages including ggplot2 for data visualisation  # <1>
library(modelsummary)                                                         # <1>
library(marginaleffects)                                                      # <1>
library(rstanarm) # Bayesian inference                                        # <1>
library(ggdist)                                                               # <1>
library(ggtext)                                                               # <1>
library(gridExtra)                                                            # <1>
library(modelsummary)                                                         # <1>
library(kableExtra)                                                           # <1> 
library(gt)                                                                   # <1>
library(rstantools)                                                           # <1>
library(tidybayes)                                                            # <1>
library(patchwork)                                                            # <1>
library(Hmisc)                                                                # <1>
library(broom.mixed)                                                          # <1>
library(janitor)                                                              # <1>
library(flextable)                                                            # <1>
library(gtsummary)                                                            # <1>
```

1.  The first lines of any `R` script usually add packages from your library into your current work session with the `library` function. If you do not have one or more of these packages installed you will need to first run `install.packages(abd)` etc or alternatively, if the package is on`CRAN` (Comprehensive R Archive Network's 21,000 packages) use the package -\> install button in R Studio's lower right window and follow the instructions.

The data added.

[**R Code 2.2** Import the data]{.aside}
---
code-annotations: select
---

```{r}
#| message: false
#| code-summary: "R Code 2.2"
#| code-block-bg: true
#| highlight-style: github
lion_noses <- as.data.frame(LionNoses)                     # <1>
```

1.  Load `LionNoses` from the `abd` package into a data.frame

In your R Studio session, two things have changed. In the top right window under the `Environment` tab, the data is now loaded and available for analysis, with a description of the number of observations (measured lions) and variables. Click on the blue button in the left and the variables are listed, with their names and type (numerical). If you click on the white rectangle to the right of the text, the full data set appears as a file in the top right window, in a form known as a dataframe in R. The data is in the correct "tidy" format for analysis; one observation (lion) per row and each variable as a column. Hover over the column title and there is a summary of the variable type and range of values. The data is ready for analysis, which is reassuring. Often if acquiring data from someone else, and especially someone who uses Excel, tidying the data to get it in a format for analysis in R or any other statistical software can be a major piece of work.

```{r}
#| message: false
#| code-summary: "Code 3.3 Look at the data"
#| code-block-bg: true
#| highlight-style: github

skimr::skim(lion_noses)
```


As our question is whether there is a relation between lion age and nose markings, a suitable exploratory visualisation is a scatterplot of these two variables. By convention, the outcome variable (age) is put on the y axis and the predictor (nose marking) on the x axis. The nose markings are reported as proportions in the original data. Percentages can be more familiar, so lets create a new variable, `percentage.black` by multiplying the proportions by 100 then use this variable in the plot and subsequent analysis (`Code 3.3` below).

---
code-annotations: select
---

```{r fig.cap='Figure 2.1 Scatterplot of lion nose markings and age, from [@whitman2004].'}
#| label: fig-cap-margin
#| fig-cap: "Scatterplot of lion nose markings and age"
#| cap-location: margin
#| message: false
#| code-summary: "Code 3.3 Create new variable and plot lion nose data"
#| code-block-bg: true
#| highlight-style: github
lion_noses <- lion_noses |>
  mutate(percentage.black = 100*proportion.black)         # <1> 

ggplot(lion_noses, aes(percentage.black, age)) +          # <2>
  geom_point() +                                          # <3>
 theme_bw() +                                             # <4>                                  
  xlab("Percentage of nose black") +                      # <5>
  ylab("Age (years)") +                                   # <5>
    annotate("rect", xmin = 5, xmax = 35, ymin = 0.5,     # <6>      
             ymax = 4.8, alpha = .1,fill = "blue") +      # <6>
    annotate("text", x = 22, y = 0, label = "91% of lions under 5 years have 40% or less nose black", colour = "blue", size = 3) +                 # <6>
    annotate("rect", xmin = 41, xmax = 80, ymin =5,       # <6>
             ymax = 9, alpha = .1,fill = "green") +
    annotate("text", x = 60, y = 9.6, label = "70% of lions over 5 years have more than 40% nose black", colour = "green", size = 3)              # <6>
```

1.  Two things are happening here. First the lion_noses data is being "piped" with the pipe operator `|>` into the second line, so we don't have to specify it again. Then, the `mutate` function is used to create a new variable column named percentage.black by multiplying (`*`) proportion.black by 100.\
2.  Create a ggplot object with the `lion_noses` data, with the predictor variable `proportion.black` on the x axis and the outcome `age` on the y axis. The `aes` function, abbreviated from aesthetic, specifies the graph layout and other features, in this case simply which variables are to be on the x and y axes, in that sequence.
3.  Add a scatterplot of the data with `geom_point`
4.  Change the ggplot theme to black & white, see [here](https://ggplot2.tidyverse.org/reference/ggtheme.html) for other options
5.  Specify the x and y axis labels with `xlab` and `ylab`
6.  With the `ggplot2` function `annotate` add boxes and text inside the plot. Note that the position of both uses the values of the variables on the x and y axes

The scatterplot above (from Code 3.3) shows that there is a trend of increased black on the nose with age, but with variability and with fewer samples for older lions.

add quote on stats being extracting meaning/information from data, but add best possible information, so the meaning is in the model, ble/green box example just a poor model. he non model is usggesting all we need to know is in the data, whilst we know it is imprecise (sampling variability) and the meaning is in a model. Model vs non model.

### Step 5. Model formulation and inference

Certainly there is, with variability, an increase in nose markings with age. The first model fit, which we may complicate later, is to suggest a straight line fit. As our data have only one predictor this is a **simple linear regression** with nose markings predicting age.

The model, let's call it ***m1***, is:

y = intercept + slope*percentage.black + error

If you remember WASSCE maths, this is the straight line formula `y = mx + c`, where m is the slope of the straight line and c is the intercept where the line meets the y axis with the addition of what statisticians call, unfortunately, error. In fact error is anything that may account for uncertainty and so an imperfect fit. This could be measurement error on the nose markings, but also other variables, not measured and so not in this model, that may influence nose markings, for example the lifetime health of a lion. It is unlikely can nose marking will perfectly predict age, like tree rings.

Slope and Intercept are **parameters**, for which m1, constrained as a straight line model, will select measures, given the data, which are in some mathematical sense a best fit of the data to the line. 

At this stage, I do not want to complicate things with discussing whether classical or Bayesian inference should be used. We shall dodge the issue for now by running the model with the Bayesian inference package `rstanarm` with flat priors, which is identical to a classical fit. This may mean nothing, but don't worry for now. We will return to this.  Let's run the model with (`Code 3.4`) and print the R output (`Code 3.5`). Read carefully the code annotation note in Code 3.4, which describes the R syntax for writing a model. 

---
code-annotations: select
---

```{r}
#| code-summary: "Code 3.4 Simple linear model"
#| output: false
#| code-block-bg: true
#| highlight-style: github

m1 <- stan_glm(age ~ percentage.black, data = lion_noses,                     # <1> 
               prior_intercept = NULL, prior = NULL, prior_aux = NULL)        # <1>                                                


```

1. This is our first specification of a model in R, so worth looking at in detail, as most R modelling code is similar. The function `stan_glm` names the inferential method, in this case Bayesian inference from the `rstnarm` package. The tilde `~` separates the left and right sides of the equation. To the left is the outcome (age) and to the right the predictors, in this case just percentage.black. The data to be used is specified, usually from the name of a file loaded into the R Studio environment. There may then be extra ***arguments***, in this example specifying the priors to make the Bayesian model identical to a classical model. The assigment symbol `<-` saves the output of the model as an object `m1` which is visible in the R Studio environment. 

some text

```{r}
#| code-summary: "Output from r packages broom.mixed and flextable"
m1_table <- broom.mixed::tidy(m1, effects = c("fixed", "aux"), 
                  conf.int = TRUE, conf.level = 0.90)

m1_table <- m1_table |> 
    slice(1:3)

ft1 <- flextable(m1_table) |>
    theme_vanilla() |>
    colformat_double(digits = 2
                     ) |>
    set_header_labels(values = list(
    term  = "parameters",
    conf.low = "5%",
    conf.high = "95%")) |>
     add_header_row(colwidths = c(3, 2),
                    values = c("", "90% CIs")) |>
                        align(align = "center", part = "all")


ft1
```


## Modelling process (following ROS chs 6-9)

::: {.callout-note collapse="false"}
### ROS Ch 6 summary 
At a purely mathematical level, the methods described in this book have two purposes: prediction and comparison. We can use regression to predict an outcome variable, or more precisely the distribution of the outcome, given some set of inputs. And we can compare these predictions for different values of the inputs, to make simple comparisons between groups, or to estimate causal effects, a topic to which we shall return in Chapters 18–21. In this chapter we use our favoured technique of fake-data simulation to understand a simple regression model, use a real-data example of height and earnings to warn against unwarranted causal interpretations, and discuss the historical origins of regression as it relates to comparisons and statistical adjustment.
:::

::: {.callout-note collapse="false"}
### ROS Ch 7 summary 
As discussed in Chapter 1, regression is fundamentally a technology for predicting an outcome y from inputs x 1, x 2, . . . . In this chapter we introduce regression in the simple (but not trivial) case of a linear model predicting a continuous y from a single continuous x, thus fitting the model yi = a + bx i + error to data (x i, yi ), i = 1, . . . , n. We demonstrate with an applied example that includes the steps of fitting the model, displaying the data and fitted line, and interpreting the fit. We then show how to check the fitting procedure using fake-data simulation, and the chapter concludes with an explanation of how linear regression includes simple comparison as a special case.
:::

::: {.callout-note collapse="false"}
### Ros Ch8 summary
Most of this book is devoted to examples and tools for the practical use and understanding of regression models, starting with linear regression with a single predictor and moving to multiple predictors, nonlinear models, and applications in prediction and causal inference. In this chapter we lay out some of the mathematical structure of inference for regression models and some algebra to help understand estimation for linear regression. We also explain the rationale for the use of the Bayesian fitting routine `stan_glm` and its connection to classical linear regression. This chapter thus provides background and motivation for the mathematical and computational tools used in the rest of the book.
:::

Chapter 7 sequence

- read in the data, having tidied first if necessary
  - made easy on this occasion as the data is in an R package and is tidy (one row per observation, one column per variable)
- define your model
  - For the first and simplest model (m1) assume there is a straight line relation between the outcome (age) and the predictor (nose markings), though with uncertainty ("error") due to one or all of sampling error, measurement error and model error. Our model is therefore the straight line equation 'y = a + bx' plus error, where a is the intercept (where the line meets the y axis) and x is the slope. So, `age = a + b*percentage.black + error`    
- convert the model into R code
  - There are several inference machines in statistics, all available in R. We'll use Bayesian inference via the user-friendly `rstanarm` package, but regularly cross-reference examples to classical inference implements with the `lm` function of R's base package `stats`. With rstanarm our first line of code is `m1 <- stan_glm(age ~ percentage.black,` `data = lion_noses)` see `Code 3.4` for more explanation
***Note*** To make rstanarm produce same as `lm` the code is `stan_glm(age ~ percentage.black, data=` `lion_noses, prior_intercept=NULL, prior=NULL,` `prior_aux=NULL, algorithm="optimizing")`

- run your model and look at the results
  - either print as in code 3.5 or use the `modelsummary` package
  
```{r}
#| warning: false
modelsummary(m1, gof_map = NA, statistic = 'conf.int', conf_level = 0.9, shape = term ~ model + statistic)
```
  
| parameter  | estimate  | uncertainty  |
|------------|-----------|--------------|
| a          | 0.872     | 0.603        |
| b          | 0.107     | 0.016        |
| $\sigma$   | 1.713     | 0.227        |

: parameters {.striped .hover}

These can also be calculated "by hand" from the posterior simulations

```{r}
#| code-summary: "parameters and uncertainty from model matrix (use array?) of posterior simulations"
sims <- as.matrix(m1)
estimate_a <- round(median(sims[,1]),3) 
estimate_b <- round(median(sims[,2]),3)
estimate_c <- round(median(sims[,3]),3)
ci_a <- round(quantile(sims[,1], c(0.025, 0.975),3))
ci_b <- round(quantile(sims[,2], c(0.025, 0.975),3))

printL(
"the estimated intercept is" = estimate_a,
"the estimated slope is"=estimate_b,
"the estimated residual standard deviation is"=estimate_c,
"the intercept cis are"=ci_a,
"the slope cis are"=ci_b
)
```

So the result for $\sigma$ the residual standard deviation means that age will be within +/- 2*1.7 = +/-3.4 years 95% of the time.

The R<sup>2</sup> is given by `R2 <- 1 - sigma(m1)^2 / sd(lion_noses$age)^2`

```{r}
r2 <- 1- sigma(m1)^2 / sd(lion_noses$age)^2
print(r2)
```

So almost 60% of the variance in age can be explained by nose markings. Where is the other 40%? Probably in other variables (lion health?) the model did not include. 
---
code-annotations: select
---

```{r class.output = "numberLines lineAnchors"}
#| code-summary: "Code 3.5 model 1 output"
#| code-block-bg: true
#| highlight-style: github
#| source-line-numbers: "1"
#| class-output: "highlight numberLines"
#| output-line-numbers: "7-9,12-13"
print(m1, digits = 3)
```

```{r class.output = "numberLines lineAnchors"}
#| code-summary: "Code 3.6 model 1 summary"
#| code-block-bg: true
#| highlight-style: github
#| source-line-numbers: "1"
#| class-output: "highlight numberLines"
#| output-line-numbers: "13-16"
summary(m1, digits = 3)
```
When the model m1 is run, a large amount of information is saved into m1, now visible in the R Studio environment. The essentials for interpreting the model are shown above, with the `print(m1)` command, but we could also look at any of the 27 stored objects individually, most of which we will not use, for example the coefficients (lines 8-9 above, hover over the code output) with `m1$coefficients`, which gives the same information as the Median column in the summary, although to more decimal places. This option to look more carefully at the output, rather than be presented with just a summary, is distinctive to R and other coding softwares.

The output is telling us (lines 7-9 highlighted), for now ignoring statistical uncertainty, that the best fit to the proposed model `y = intercept + slope*percentage.black + error` is:


<p style="text-align: center;"><b>age = 0.9 + 0.1*percentage.black</b></p>


Let's plot this line and the data.

---
code-annotations: select
---

```{r}
#| message: false
#| code-summary: "Code 3.6 A plot of the simple linear regression for model 1"
#| code-block-bg: true
#| highlight-style: github
p1 <- ggplot(lion_noses, aes(percentage.black, age)) +
    geom_point() +
    geom_abline(intercept = 0.9, slope = 0.1) +
    theme_bw() +
    xlab("percentage black on nose") +
    ylab("age (years)")

df <- data.frame(
  x = 26,
  y = 5,
  label = "y = 0.9 + 0.1x"
)

p1 + geom_label(data = df, aes(x,y, label = label),
                 color = "black", size = 5) +
    xlim(0, 80)
```

Let's plug some numbers into the equation. Three lions L1, L2 and L3 are photographed in the field and subsequent investigation of the image estimates 42%, 50% and 60% nose markings. Using R as a pocket calculator,  L1's estimated age is given by the R code `0.9 + 0.1*42` = 5.1 years, L2's `0.9 + 0.1*50` = 5.9 years and L3's `0.9 + 0.1*60` = 6.9 years. As our model is a monotonic line (no curves), change in y as determined by the slope is constant and the general result is that;

**A 10% increase in nose markings equates to a 1 year increase in age**.

**A safe interpretation is that, under model assumptions, the average difference in age, comparing two lions 10% different in nose markings, is one year. The safest interpretation of a regression is as a comparison**. a nod towards effects and causal inference.

To summarize: regression is a mathematical tool for making predictions. Regression coefficients can sometimes be interpreted as effects, but they can always be interpreted as average comparisons.

If our model is correct, which we shall need to check, it is a better method of selecting which lions to hunt for trophies than the ad-hoc model, which assumed a lion with 40% nose markings was under 5 years. L1 met that threshold, but only just. If our measurer had been in a hurry and making small mistakes, whereas the actual measure was 40%, L1's age should have been modelled as `0.9 +0.1*40` = 4.9 years and he could have not been made available to trophy hunters. CHECK THIS    

The standard error for the slope (MAD_SD on line 9 of the model output `Code 3.5`) is 0.015, so the model has a slope of, to two decimal places, 0.11 +/- 0.01 and the 95% interval

```{r}
#| message: false
#| code-summary: "Code 3.7 Print the Confidence Intervals for model 1"
#| code-block-bg: true
#| highlight-style: github
round(posterior_interval(m1, prob = 0.9), 2)
```

::: {.callout-note collapse="false}
### ROS Ch 9
Bayesian inference involves three steps that go beyond classical estimation. First, the data and model are combined to form a posterior distribution, which we typically summarize by a set of simulations of the parameters in the model. Second, we can propagate uncertainty in this distribution—that is, we can get simulation-based predictions for unobserved or future outcomes that accounts for uncertainty in the model parameters. Third, we can include additional information into the model using a prior distribution. The present chapter describes all three of these steps in the context of examples capturing challenges of prediction and inference.
:::

For now we'll not worry about priors and use defaults as per *Bayes Rules!*, complicated and we have no prior information to feed into the model, but concentrate on the posterior simulations to get estimates.

After fitting a regression, y = a + bx + error, we can use it to predict a new data point, or a set of new data points, with predictors x new . We can make three sorts of predictions, corresponding to increasing levels of uncertainty:

1. The ***point prediction***, $\hat{a}$ + $\hat{b}$x new : Based on the fitted model, this is the best point estimate of the average value of y for new data points with this new value of x. We use $\hat{a}$ and $\hat{b}$ here because the point prediction ignores uncertainty.

2. The ***linear predictor*** with uncertainty, a + bx new , propagating the inferential uncertainty in (a, b). This represents the distribution of uncertainty about the expected or average value of y for new data points with predictors x new .

3. The ***predictive distribution for a new observation***, a + bx new + error: This represents uncertainty about a new observation y with predictors x new .

Consider our study, with y (age) predicted from x (nose markings). 

<!-- 1. For any given x new , the point prediction is the best estimate of the lion's age in the population, conditional on mose markings x new.  -->

2. The linear predictor is the modeled average age of lions with nose marking x new in the population, with uncertainty corresponding to inferential uncertainty in the coefficients a and b.

3. The predictive distribution represents the age of a single lion drawn at random from this population, under the model conditional on the specified value of x new .

As sample size approaches infinity, the coefficients a and b are estimated more and more precisely, and the uncertainty in the linear predictor approaches zero, but the uncertainty in the predictive distribution for a new observation does not approach zero; it approaches the residual standard deviation $\sigma$ (sigma in Table 1).

Consider a lion on territory, so it can be re-found for trophy hunting, which is photographed and the nose markings measured as 45%. Let's calculate the three measures for this individual.

#### Point estimate

The point estimate where the best fit regression line can be approximately read off the graph `Code 3.6` as the value of y where x = 45. It is calculated with the base r function `predict`

```{r}
new <- data.frame(percentage.black=45)
y_point_pred <- round(predict(m1, newdata=new), 1)
cat("The point prediction of the average age in years of a lion with \n45% nose markings is", y_point_pred)
```

#### Linear predictor

To put uncertainty intervals around the point estimate, the rstanarm function `posterior_linpred` is used

```{r}
y_linpred <- round(posterior_linpred(m1, newdata=new), 2)
ci_b <- quantile(y_linpred, c(0.05, 0.95))
cat("The linear predictor of the average age in years \nof a lion with 45% nose markings are given by the \n90% uncertainty intervals", ci_b)
```

#### Predictive uncertainty

This uses the rstanarm function `posterior_predict`

```{r}
y_pred <- posterior_predict(m1, newdata=new)
Median <- median(y_pred)
MAD_SD <- mad(y_pred)
young <- mean(y_pred < 5)
uncertainty_90 <- quantile(y_pred, c(0.05, 0.95))
printL("The predicted age of a lion with 45% nose markings is"=round(Median,1),
  "with uncertainty intervals"=round(uncertainty_90, 1),
  "The probability that a lion with 45% nose marking is under \n5 years' age is" =round(young, 2)
  )
```

It is also possible to calculate any of these three over a range of values. Let's do that for the range of nose markings for which we have data (a rule of thumb to not predict outside of the data range) so between 10-80% and at 5% intervals

```{r}
new_grid <- data.frame(percentage.black=seq(10, 80, 5))
y_point_pred_grid <- predict(m1, newdata=new_grid)# 15 values
y_point_pred_df <- as.data.frame(y_point_pred_grid)
y_linpred_grid <- posterior_linpred(m1, newdata=new_grid) # 4000 sims for each of 15 values
y_linpred_cis <- as.data.frame(y_linpred_grid) 
y_pred_grid <- posterior_predict(m1, newdata=new_grid) # 4000 sims for each of 15 values
y_pred_cis <- as.data.frame(y_pred_grid)

quants <- c(0.05,0.95)
linear_uncertainty <- apply( y_linpred_cis[1:15] , 2 , quantile , probs = quants , na.rm = TRUE )
prediction_uncertainty <- apply( y_pred_cis[1:15] , 2 , quantile , probs = quants , na.rm = TRUE )

linear_uncertainty_t <- t(linear_uncertainty)
prediction_uncertainty_t <- t(prediction_uncertainty)

intervals <- as.data.frame(seq(10, 80, 5))

m1_estimates <- bind_cols( intervals, y_point_pred_df, linear_uncertainty_t, prediction_uncertainty_t)
# to complete

gt_tbl <- gt(m1_estimates)
gt_tbl |>
    tab_header(
        title = "Estimates of point, point uncertainty and prediction uncertainty for lion age from nose markings",
        subtitle = "Estimates at 5% intervals for nose markings over the range of the sample data"
    ) |> 
    fmt_number(
        decimals = 1
    ) |> 
    cols_label(
    "seq(10, 80, 5)" = "Nose markings (percent)",
    y_point_pred_grid = "point prediction of mean age (years)",
    "5%...3" = "5%",
    "95%...4" = "95%",
    "5%...5" = "5%",
    "95%...6" = "95%"
  ) |> 
    tab_spanner(
    label = "uncertainty intervals for mean age",
    columns = c(3:4)
  ) |> 
    tab_spanner(
    label = "uncertainty intervals for individual age prediction",
    columns = c(5:6)
  ) |> 
    cols_align(
    align = "center",
    columns = c(1:6)
  )
```



```{r}
#| code-summary: "estimates from simulation matrix"
sims <- as.matrix(m1)
Median <- round(apply(sims, 2, median),3)
MAD_SD <- round(apply(sims, 2, mad),3)
print(cbind(Median, MAD_SD))
       
```




```{r}
#| code-summary: "Code 3.9 A comparison of classical and Bayesian models"
#| code-block-bg: true
#| output: false
#| highlight-style: github
models <- list(
  "OLS classical" = lm(age ~ percentage.black, data = lion_noses),
  "OLS rstanarm default priors" = stan_glm(age ~ percentage.black, data = lion_noses),
  "OLS rstnarm flat priors" = stan_glm(age ~ percentage.black, data = lion_noses, prior_intercept = NULL, prior = NULL, prior_aux = NULL)
)
```

```{r}
#| code-summary: "Code 3.10 A comparison of classical and Bayesian models"
#| code-block-bg: true
#| highlight-style: github
#| warning: false
modelsummary(models, gof_map = NA, statistic = 'conf.int', conf_level = 0.95, stars = TRUE, shape = term ~ model + statistic)
```

::: callout-note
Note that there are five types of callouts, including: `note`, `warning`, `important`, `tip`, and `caution`.
:::

::: callout-tip
## Tip with Title

This is an example of a callout with a title.
:::

```{r}
#| message: false
#| code-summary: "Code 3.11 Plot mean uncertainty"
lion_noses %>%
    tidybayes::add_epred_draws(m1) %>%
    ggplot(aes(x = percentage.black, y = age)) +
    stat_lineribbon(aes(y = .epred), .width = c(0.5, 0.9), color = "#08519C") +
    geom_point(data = lion_noses) +
    scale_fill_brewer()
```

```{r}
#| message: false
#| code-summary: "Code 3.12 Plot individual uncertainty"
lion_noses %>%
    tidybayes::add_predicted_draws(m1) %>%
    ggplot(aes(x = percentage.black, y = age)) +
    stat_lineribbon(aes(y = .prediction), .width = c(0.5, 0.9), color = "#08519C") +
    geom_point(data = lion_noses) +
    theme_bw() +
    scale_fill_brewer()
```


```{r}
#| message: false
#| warning: false
#| code-summary: "Code 3.13 Plot mean and individual uncertainty combined"
p1 <- lion_noses |> 
    tidybayes::add_epred_draws(m1) |> 
    ggplot(aes(x = percentage.black, y = age)) +
    stat_lineribbon(aes(y = .epred), .width = c(0.5, 0.9), color = "#08519C") +
    geom_point(data = lion_noses) +
    scale_fill_brewer()
p2 <- lion_noses |>     
    tidybayes::add_predicted_draws(m1) |> 
    ggplot(aes(x = percentage.black, y = age)) +
    stat_lineribbon(aes(y = .prediction), .width = c(0.5, 0.9), color = "#08519C") +
    geom_point(data = lion_noses) +
    theme_bw() +
    scale_fill_brewer()

p3 <- lion_noses |> 
    tidybayes::add_epred_draws(m1)

p4 <- lion_noses |> 
    tidybayes::add_predicted_draws(m1)

p5 <- bind_rows(p3, p4)

p5 |> 
ggplot(aes(x = percentage.black, y = age)) +
    stat_lineribbon(aes(y = .epred), .width = c(0.95), color = "#08519C") +
    stat_lineribbon(aes(y = .prediction), .width = c(0.9), color = "#08519C", alpha = 0.2) +
    geom_point(data = lion_noses) +
    theme_bw() +
    scale_fill_brewer() +
    guides(fill = FALSE)
```


```{r}
#| column: screen-inset-shaded
#| message: false
#| warning: false
#| code-summary: "Code 3.14 Plot mean and individual uncertainty side by side"
p1 / p2
```

## Bayes Rules modelling sequence; from Chapter 9

### specify the data model

equation. Maths is shorthand, translate into english

$Y_i|\beta_0,\beta_1,\sigma$ $\simeq$ $N(\mu_1, \sigma^2)$ with $\mu_1 = \beta_0 +\beta_1X_1$

- coefficients and parameters alternative names. 
- add fig 9.2 (simulate) to show how same parameters for slope and intercept ca arise with different sigmas. 

### Specify the priors

Explain why defaults in rstnarm are ok 

### Posterior simulation

```{r}
bayesplot::neff_ratio(m1)
```

```{r}
bayesplot::rhat(m1)
```

```{r}
bayesplot::mcmc_trace(m1, size = 0.1)
```

```{r}
bayesplot::mcmc_dens_overlay(m1)
```

```{r}
lion_noses %>%
  add_fitted_draws(m1, n = 100) %>% 
  ggplot(aes(x = percentage.black, y = age)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + 
    geom_point(data = lion_noses, size = 0.05)
```



## Present conclusions

yaba yaba

## Interpret in subject matter terms 

#### Shiny app

It is often useful in communicating your results to have an interactive app that can, in this case, allow someone to enter a nose marking measure and get output on the lion's age with the model hidden. This has become increasingly easy in R with [Shiny](https://shiny.posit.co/). We'll follow the official Shiny website and also a nice example on spatio-temporal data from [@Morega_2020] which is [free online](https://www.paulamoraga.com/book-geospatial/) 

### Notes on stuff to add/added 

added quarto [line-highlight](https://github.com/shafayetShafee/line-highlight#line-highlight-extension-for-quarto) extension for highlighting code, including output code and [fontawesome](https://github.com/quarto-ext/fontawesome) for icons

final section shiny app??

Check Paula Morega

add labels to code chunks [cv quarto template](https://github.com/kazuyanagimoto/quarto-awesomecv-typst)

add [targets](https://books.ropensci.org/targets/literate-programming.html)

[sam csik's blog](https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/) article has instructions and code, how to add footnotes and a bibliography and citations and how to populate margins. Use these. also recommends [W3 schools for css/scss/html](https://www.w3schools.com/cssref/pr_font_weight.php)

### refs and open books to add

[R for Data Science 2e](https://r4ds.hadley.nz/) and [source code on GitHub](https://github.com/hadley/r4ds)

[Tidy Modeling with R](https://www.tmwr.org/)

[R Markdown Cookbook online](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html)

[ggplot2 3e online](https://ggplot2-book.org/)

[R Graphics Cookbook online](https://r-graphics.org/) ggplot2 based

[Bayes Rules! An Introduction to Applied Bayesian Modelling](https://www.bayesrulesbook.com/) uses rstanarm

[Frank Harrell's R workflow, inlcuding Quarto](https://hbiostat.org/rflow/)

[Harrell's associated course](https://hbiostat.org/rmsc/)

[huxtable for regression tables, Andrew Heiss' blog](https://www.andrewheiss.com/blog/2018/03/08/amelia-broom-huxtable/)

[Huxtable R table package](https://hughjonesd.github.io/huxtable/huxtable.html) recommended for regression tables

[patchwork r package](https://patchwork.data-imaginist.com/articles/patchwork.html) to reorganise image layout

[html validator](https://validator.w3.org/)

[LaTex symbols](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols)

[efficient R programming](https://csgillespie.github.io/efficientR/)

[modern dive](https://moderndive.com/index.html)
