[
  {
    "objectID": "linux-posts.html",
    "href": "linux-posts.html",
    "title": "Linux resources",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 24, 2022\n\n\nSimple linear regression\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paul Robinson",
    "section": "",
    "text": "0.1 Me\nI am a conservation biologist, with a background in statistics, based in Sierra Leone for half of the year and otherwise working remotely from the UK.\nMy employment comprises three part-time posts: data scientist for the charity World Parrot Trust Central and West African projects, with particular emphasis on Timneh and Grey Parrots; managing biodiversity projects at Outamba-Kilimi National Park in Sierra Leone for the charity and national NGO Pan Verus Project; and, as Visiting Lecturer, teaching and supervising data science/statistics to postgraduate students and professionals through the Department of Biological Sciences, Fourah Bay College, University of Sierra Leone.\nA new venture, with both Pan Verus Project and World Parrot Trust, is the development of wildlife tourism in Sierra Leone.\n\n\n0.2 What is on this website\nI maintain two blogs here: data science/stats applied mainly to conservation biology, mostly using the R language, aimed at people who do not want calculus and matrix algebra, but are willing to code and think; and wildlife sightings and resources to make finding and enjoying birds and other wildlife in Sierra Leone easier.\nIn addition, there are notes written as much to me as to others, on maintaining and using my Linux laptop and an assortment of Free and Open Source Software.\n\n\n0.3 Research and other professional interests\nMy interests, mostly with birds, but drifting into other vertebrate fauna and also flora where the opportunity, usually through student projects, arises, are:\n\nModelling species’ distribution, abundance and niche;\nhuman-wildlife interaction and conflicts;\nspatial and temporal modelling of land use and species’ change, including applications of GIS; and\nacoustic species’ identification and monitoring.\n\n\n\n0.4"
  },
  {
    "objectID": "sl-wildlife-posts/2024-08-19-simple-linear-regression/index.html",
    "href": "sl-wildlife-posts/2024-08-19-simple-linear-regression/index.html",
    "title": "Birding from my balcony",
    "section": "",
    "text": "CitationBibTeX citation:@online{2025,\n  author = {},\n  title = {Birding from My Balcony},\n  date = {2025-08-08},\n  url = {https://paulauvirage.github.io/posts/2024-08-19-simple-linear-regression/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Birding from My Balcony.” 2025. August 8, 2025. https://paulauvirage.github.io/posts/2024-08-19-simple-linear-regression/."
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html",
    "title": "2: Simple linear regression",
    "section": "",
    "text": "To start, click on the arrow in the “Document structure” box, below."
  },
  {
    "objectID": "wildlife-tourism.html",
    "href": "wildlife-tourism.html",
    "title": "Wildlife tourism",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "statsposts.html",
    "href": "statsposts.html",
    "title": "Data science blog",
    "section": "",
    "text": "Course introduction\n\n\n\n\nR\n\n\nregression\n\n\n\nWhat is statistics and how to implement it with careful thinking, R and R Studio\n\n\n\n\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2: Simple linear regression\n\n\n\nR\n\n\nregression\n\n\n\nAn introduction to statistical modelling and inference via regression on one outcome and one predictor\n\n\n\nPaul Robinson\n\n\nAug 17, 2024\n\n\n\n\n\n\n:::\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "sl-wildlife-posts.html",
    "href": "sl-wildlife-posts.html",
    "title": "Sierra Leone Wildlife blog",
    "section": "",
    "text": "Birding from my balcony\n\n\n\nbirding\n\n\n\nA year of breakfast birding in suburban Freetown\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statsposts/2024-08-18-course-introduction/index.html",
    "href": "statsposts/2024-08-18-course-introduction/index.html",
    "title": "1. Course introduction",
    "section": "",
    "text": "To start, click on the arrow in the “Document structure” box, below.\n\n\n\n\n\n\nDocument structure\n\n\n\n\n\nThis tutorial assumes that you are working in R Studio.\nAs with all posts in this group, the main text is interspersed with two types of blocks.\n“Call-outs” of supporting information, such as this one, are folded away to avoid their interfering with a first read of the post. I recommend reading them all. You can unfold them by clicking on the far right arrow inside the box.\nCode blocks, with a grey background and monotype font, are not folded away. They allow you to implement all of the analysis in the post on your computer. You can cut and paste the code in each code block into your own R scripting window in R Studio, by clicking on the clipboard in the left of the block, but I recommend your typing it in, to get practice in how R code works in R Studio.\n\n\n\n\nWorking example - trophy hunting of lions\nTrophy hunting of lions, particularly males, is a controversial, but active, means of conserving populations. Selective killing of older lions is thought to result in fewer knock-on deaths, as older animals are less likely to be breeding and the killing of a younger breeding male can result in further lion deaths, particularly through infanticide, as a new male replaces the killed male in the pride. The amount of black on a lion’s nose is the best remote indicator of age. This measure has been taken for lions of known age in a long term study at the Serengeti, Tanzania. The data is available for re-analysis in the R package abd and has been used for an example of linear regression in the online book Statistics for ecologists .\n\n\n\n\n\n\nThe advantages of open data\n\n\n\n\n\nThis worked example is possible because the data has been made easily, publicly accessible. It is not essential, though it is helpful, that it is accessible through an R package. In a later post an optimal workflow for scientists will be discussed in which open data is a component.\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\nThe objectives of regression\nLet’s review what regression aims to do.\n\n\n\n\n\n\nThe objectives of regression\n\n\n\n\n\nRegression is the most widely used method of statistical inference, generalising from the sample on which measurements have been taken to the population for which a hypothesis or, more often and more usefully, the size and direction of effects, is sought.\nThe sample data contains, for each unit on which measurements have been taken, the outcome it is hope to explain and one or more predictors which it is hoped can explain variability in the outcome.\nRegression is\n\n\n\nLoad the required packages.\n:\n\n\nQuestions for regression\nhypothesis testing (is there a relation between age and black nose) and regression (size and direction of effect of age on black nose; sampling and measurement uncertainty) including prediction (if we photo a lion of unknown age on its territory, how good (accurate and precise) is this model as a predictor of its age? Good enough to be implemented in awarding shooting permits?)\n\n\n\n\n\n\nPackage management in R\n\n\n\n\n\n\n\n\n\n\n\n\nImport packages to session\n\nImport packages into current session\n\n# run the function install.packages(\"abd\") if the package abd is not already installed  \nlibrary(abd) # Run each time you need to use the package in a new R session\n\n\n\nsam csik’s blog article has instructions and code, how to add footnotes and a bibliography and citations and how to populate margins. Use these.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\nrefs to add\nLoveridge, A.J., Wijers, M., Mandisodza-Chikerema, R. et al. Anthropogenic edge effects and aging errors by hunters can affect the sustainability of lion trophy hunting. Sci Rep 13, 95 (2023). https://doi.org/10.1038/s41598-022-25020-9 Whitman, K., Starfield, A. M., Quadling, H. S., & Packer, C. (2004). Sustainable trophy hunting of african lions. Nature, 428(6979), 175.\n\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {1. {Course} Introduction},\n  date = {2024-08-19},\n  url = {https://paulauvirage.github.io/posts/2024-08-22-course-introduction/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“1. Course Introduction.” 2024. August 19, 2024. https://paulauvirage.github.io/posts/2024-08-22-course-introduction/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "linux-posts/2024-08-19-simple-linear-regression/index.html",
    "href": "linux-posts/2024-08-19-simple-linear-regression/index.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "CitationBibTeX citation:@online{2022,\n  author = {},\n  title = {Simple Linear Regression},\n  date = {2022-10-24},\n  url = {https://paulauvirage.github.io/posts/2024-08-19-simple-linear-regression/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Simple Linear Regression.” 2022. October 24, 2022. https://paulauvirage.github.io/posts/2024-08-19-simple-linear-regression/."
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#working-example---trophy-hunting-of-lions",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#working-example---trophy-hunting-of-lions",
    "title": "2: Simple linear regression",
    "section": "1 Working example - trophy hunting of lions",
    "text": "1 Working example - trophy hunting of lions\nTrophy hunting of lions, particularly males, is a controversial, but active, means of conserving populations. Selective killing of older lions is thought to result in fewer knock-on deaths, as older animals are less likely to be breeding and the killing of a younger breeding male can result in further lion deaths, particularly through infanticide, as a new male replaces the killed male in the pride (Whitman et al. 2004), (Loveridge et al. 2023). The amount of black on a lion’s nose is the best remote indicator of age . This measure has been taken for lions of known age in a long term study at the Serengeti, Tanzania. The data is available for re-analysis in the R package abd and has been used for an example of linear regression in the online book Statistics for ecologists (Fieberg 2024).\n\n\n\n\n\n\nThe advantages of open data\n\n\n\n\n\nThis worked example is possible because the data has been made easily, publicly accessible. It is not essential, though it is helpful, that it is accessible through an R package. In a later post an optimal workflow for scientists will be discussed in which open data is a component."
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#the-objectives-of-regression",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#the-objectives-of-regression",
    "title": "2: Simple linear regression",
    "section": "2 The objectives of regression",
    "text": "2 The objectives of regression\nLet’s review what regression aims to do, applied to the lion nose data, in the call-out box below.\n\n\n\n\n\n\nThe objectives of regression\n\n\n\n\n\nRegression is the most widely used method of statistical inference, generalising from the sample on which measurements have been taken to the population for which a hypothesis or, more often and more usefully, the size and direction of effects, is sought.\nThe sample data for a regression problem, gathered for the project or reused, contains, for each unit on which measurements have been taken, the outcome it is hope to explain and one or more predictors which it is hoped can explain variability in the outcome with sufficient precision to be useful.\nRegression, and statistical inference more generally, is therefore a problem of prediction in the face of uncertainty. There is uncertainty about whether:\n\nthe sample represents the population;\nthe measurements are truly measuring the features of interest (“measurement validity”); and\nthe mathematical relation between outcome and predictor is causal.\n\nRegression usually seeks to predict the average outcome for the predictor(s). To be more explicit in applying this reasoning to the lion nose data set, the predictor is the amount of black on the nose and the outcome which it is aimed to predict is the age of a lion for which the predictor (nose black marks) has been measured, but the outcome (age) is not known.\nAn hypothesis test could establish whether or not there is a relation between measured nose markings and age, usually phrased as a null hypothesis; H0 there is no relation, versus the alternative hypothesis that H0 is not supported.\nThis course will not concern itself much with hypothesis testing, but rather the more useful measurement of effects; is there a predictable change in the amount nose marking with age?\nIt is very unlikely that every lion of a certain age will have exactly the same amount of nose markings. If this was true, you would not need to use statistics, but rather simple algebra. Variability, arising from whatever processes cause nose markings and/or measurement error, are likely to give a range of values for lions of the same age. A regression model uses this variability and the size of the sample data (number of individuals with both nose marking and age measured) to estimate and report on the uncertainty in any prediction. Depending on the method of inference used - Frequentist or Bayesian - the uncertainty is reported as confidence intervals or credible intervals which have subtly different meanings. This will be discussed more later. What is important to remember is that statistical modelling, as opposed to deterministic modelling, is: necessary when there is variability in the data; and is reported with measures of uncertainty. add important uses of regression from Gelman p5?"
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#analysis-of-the-lion-nose-data-with-r",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#analysis-of-the-lion-nose-data-with-r",
    "title": "2: Simple linear regression",
    "section": "3 Analysis of the lion nose data with R",
    "text": "3 Analysis of the lion nose data with R\n\n\n\n\n\nflowchart LR\n  A[Question] --&gt; B(Data)\n  B --&gt; C(Tidy)\n  C --&gt; D(Exploratory analysis)\n  D --&gt; E{Model}\n  E --&gt; F[Interpret]\n  E --&gt; G[Report]\n\n\n\n\n\n\n\n\n\n\n\n\nA recap of the ideal sequence of a statistical investigation\n\n\n\n\n\nWe will follow (Cox and Donnelly 2011) in defining seven steps for an applied statistical investigation that integrates the subject matter (lion nose markings and age) and statistical techniques.\n\nFormulate and clarify a research question of subject-matter importance\n\n\nDon’t waste your time on questions for which few people are interested in the answer; make the question(s) clear.\n\n\nDesign an investigation to produce secure answers.\n\n\nbbb\n\n\nProduce effective and reliable measurement procedures.\n\n\nccc\n\n\nPreliminary analysis\n\n\nOften referred to as Exploratory Data Analysis (EDA). Use simple methods with visuals and tables to check data and inform on model choise\n\n\nModel formulation and inference\n\n\nDevelop and apply a model to give answers, with some assessment of thier uncertainty\n\n\nPresent conclusions\n\n\neee\n\n\nInterpret in subject matter terms and in relation to the knowledge base of the field\n\n\nfff\n\n\n\n\n\n3.1 Step 1. Subject matter questions\nWhen possible choose an important question that is likely to be soluble from the data. The Lion is classified as globally Vulnerable. Trophy hunting is economically valuable. Therefore an investigation of how selective trophy hunting could minimise the impact on the lion population size is of importance. There is evidence that trophy hunting older lions could cause less impact on the population and that lion age can be measured unobtrusively from the amount of black marking on the nose. The broad questions are:\n\nIs there a difference in nose marking with age (hypothesis test)?\nCan the difference be modeled to provide a measure of how nose markings change with age (regression model); and\nCan the model be applied, for example to regulate permits for trophy hunting?\n\n\n\n3.2 Step 4. Exploratory data analysis (EDA), jumping Steps 2 (Study Design) and 3 (Measurement validity) as existing data is being used.\nLet’s start with question b. The data is already collected, so the usual Step 2 of designing a study to collect data and Step 3 of measurement validity are not necessary. However, you should read the papers that have been produced using the data, as how the data was gathered (both design and measurement) could affect how it can be interpreted. Maybe contact the authors if you have a query.\nA first task, and so usually the first lines of R code, is to import into the analysis session, from your local library on your computer, the necessary R packages for the data handling, manipulation and analysis (Code 3.1) Next, import the data into your session (Code 3.2).\n\n\n\n\n\n\nPackage management in R\n\n\n\n\n\nIf you are familiar with software that comes ready packaged with all of its capabilities, R may seem strange. It follows a different philosophy of bundling up one or a few related tasks into a package. When R is first downloaded onto your computer, it comes with 15 “base” packages, of the c21,000 packages available in the cloud on the Comprehensive R Archive network and others available from other cloud storage locations.\nAny analysis usually needs more than the base packages. This course will use an extra 20 or so. You can, with an internet connection, download and install any extra packages onto your computer, where they will be added to your package library (see the packages tab in R Studio’s lower right window). As developers work on packages, the changed version needs to be updated on your computer. You will not be prompted to update. In R Studio the ‘packages’ then ‘update’ tab (bottom right window) is available for this. It will, with an internet connection, search for those of your installed packages that have updates and give you the option to install the updated versions. Most packages are less than 1MB in size and few more than 5MB, so I recommend you check and update packages as often as you have a stable internet connection. I have about 100 packages and update (there usually is something to update) daily, but less often is fine.\nOnce a package is on your computer, it is not yet available to use in your current analysis session. For this you need to import the package from your library, with a line of code using the library function; for example library(marginaleffects) to import the Marginal Effects package. Repeat this for each package. In writing a script of R code for any work, you usually start with lines to import the necessary packages, as without them the subsequent code depending on them will not run.\n\n\n\nR Code 2.1 Import packages from your library into the current analysis session.\n\n\nR Code 2.1\n1library(abd) # a biological data package that includes the lion nose data\nlibrary(tidyverse) # eight packages including ggplot2 for data visualisation\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(rstanarm) # Bayesian inference\nlibrary(modelr)\nlibrary(ggdist)\nlibrary(ggtext)\nlibrary(gridExtra)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(gt)\nlibrary(rstantools)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(Hmisc)\nlibrary(broom.mixed)\nlibrary(janitor)\nlibrary(huxtable)\n\n\n\n1\n\nThe first lines of any R script usually add packages from your library into your current work session with the library function. If you do not have one or more of these packages installed you will need to first run install.packages(abd) etc or alternatively, if the package is onCRAN (Comprehensive R Archive Network’s 21,000 packages) use the package -&gt; install button in R Studio’s lower right window and follow the instructions.\n\n\n\n\nThe data added.\nR Code 2.2 Import the data\n\n\nR Code 2.2\n1lion_noses &lt;- as.data.frame(LionNoses)\n\n\n\n1\n\nLoad LionNoses from the abd package into a data.frame\n\n\n\n\nIn your R Studio session, two things have changed. In the top right window under the Environment tab, the data is now loaded and available for analysis, with a description of the number of observations (measured lions) and variables. Click on the blue button in the left and the variables are listed, with their names and type (numerical). If you click on the white rectangle to the right of the text, the full data set appears as a file in the top right window, in a form known as a dataframe in R. The data is in the correct “tidy” format for analysis; one observation (lion) per row and each variable as a column. Hover over the column title and there is a summary of the variable type and range of values. The data is ready for analysis, which is reassuring. Often if acquiring data from someone else, and especially someone who uses Excel, tidying the data to get it in a format for analysis in R or any other statistical software can be a major piece of work.\n\n\nCode 3.3 Look at the data\nskimr::skim(lion_noses)\n\n\n\nData summary\n\n\nName\nlion_noses\n\n\nNumber of rows\n32\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1\n4.31\n2.68\n1.1\n2.18\n3.50\n5.85\n13.10\n▇▃▃▁▁\n\n\nproportion.black\n0\n1\n0.32\n0.20\n0.1\n0.16\n0.26\n0.43\n0.79\n▇▃▂▁▂\n\n\n\n\n\nAs our question is whether there is a relation between lion age and nose markings, a suitable exploratory visualisation is a scatterplot of these two variables. By convention, the outcome variable (age) is put on the y axis and the predictor (nose marking) on the x axis. The nose markings are reported as proportions in the original data. Percentages can be more familiar, so lets create a new variable, percentage.black by multiplying the proportions by 100 then use this variable in the plot and subsequent analysis (Code 3.3 below).\n\n\nCode 3.3 Create new variable and plot lion nose data\n1lion_noses &lt;- lion_noses |&gt;\n    mutate(percentage.black = 100*proportion.black)\n2ggplot(lion_noses, aes(percentage.black, age)) +\n3  geom_point() +\n4    theme_bw() +\n5  xlab(\"Percentage of nose black\") +\n  ylab(\"Age (years)\") +\n6    annotate(\"rect\", xmin = 5, xmax = 35, ymin = 0.5,\n             ymax = 4.8, alpha = .1,fill = \"blue\") +\n    annotate(\"text\", x = 22, y = 0, label = \"91% of lions under 5 years have 40% or less nose black\", colour = \"blue\", size = 3) +\n    annotate(\"rect\", xmin = 41, xmax = 80, ymin =5,\n             ymax = 9, alpha = .1,fill = \"green\") +\n    annotate(\"text\", x = 60, y = 9.6, label = \"70% of lions over 5 years have more than 40% nose black\", colour = \"green\", size = 3)\n\n\n\n1\n\nTwo things are happening here. First the lion_noses data is being “piped” with the pipe operator |&gt; into the second line, so we don’t have to specify it again. Then, the mutate function is used to create a new variable column named percentage.black by multiplying (*) proportion.black by 100.\n\n\n2\n\nCreate a ggplot object with the lion_noses data, with the predictor variable proportion.black on the x axis and the outcome age on the y axis. The aes function, abbreviated from aesthetic, specifies the graph layout and other features, in this case simply which variables are to be on the x and y axes, in that sequence.\n\n3\n\nAdd a scatterplot of the data with geom_point\n\n4\n\nChange the ggplot theme to black & white, see here for other options\n\n5\n\nSpecify the x and y axis labels with xlab and ylab\n\n6\n\nWith the ggplot2 function annotate add boxes and text inside the plot. Note that the position of both uses the values of the variables on the x and y axes\n\n\n\n\n\n\n\n\n\n\nFigure 1: Scatterplot of lion nose markings and age\n\n\n\n\n\nThe scatterplot above (from Code 3.3) shows that there is a trend of increased black on the nose with age, but with variability and with fewer samples for older lions.\nadd quote on stats being extracting meaning/information from data, but add best possible information, so the meaning is in the model, ble/green box example just a poor model. he non model is usggesting all we need to know is in the data, whilst we know it is imprecise (sampling variability) and the meaning is in a model. Model vs non model.\n\n\n3.3 Step 5. Model formulation and inference\nCertainly there is, with variability, an increase in nose markings with age. The first model fit, which we may complicate later, is to suggest a straight line fit. As our data have only one predictor this is a simple linear regression with nose markings predicting age.\nThe model, let’s call it m1, is:\ny = intercept + slope*percentage.black + error\nIf you remember WASSCE maths, this is the straight line formula y = mx + c, where m is the slope of the straight line and c is the intercept where the line meets the y axis with the addition of what statisticians call, unfortunately, error. In fact error is anything that may account for uncertainty and so an imperfect fit. This could be measurement error on the nose markings, but also other variables, not measured and so not in this model, that may influence nose markings, for example the lifetime health of a lion. It is unlikely can nose marking will perfectly predict age, like tree rings.\nSlope and Intercept are parameters, for which m1, constrained as a straight line model, will select measures, given the data, which are in some mathematical sense a best fit of the data to the line.\nAt this stage, I do not want to complicate things with discussing whether classical or Bayesian inference should be used. We shall dodge the issue for now by running the model with the Bayesian inference package rstanarm with flat priors, which is identical to a classical fit. This may mean nothing, but don’t worry for now. We will return to this. Let’s run the model with (Code 3.4) and print the R output (Code 3.5). Read carefully the code annotation note in Code 3.4, which describes the R syntax for writing a model.\n\n\nCode 3.4 Simple linear model\n1m1 &lt;- stan_glm(age ~ percentage.black, data = lion_noses,\n               prior_intercept = NULL, prior = NULL, prior_aux = NULL)\n\n\n\n1\n\nThis is our first specification of a model in R, so worth looking at in detail, as most R modelling code is similar. The function stan_glm names the inferential method, in this case Bayesian inference from the rstnarm package. The tilde ~ separates the left and right sides of the equation. To the left is the outcome (age) and to the right the predictors, in this case just percentage.black. The data to be used is specified, usually from the name of a file loaded into the R Studio environment. There may then be extra arguments, in this example specifying the priors to make the Bayesian model identical to a classical model. The assigment symbol &lt;- saves the output of the model as an object m1 which is visible in the R Studio environment.\n\n\n\n\nsome text\n\n\nOutput from r packages broom.mixed and gt\nm1_table &lt;- broom.mixed::tidy(m1, effects = c(\"fixed\", \"aux\"), \n                  conf.int = TRUE, conf.level = 0.90)\n\nm1_table &lt;- m1_table |&gt; \n    slice(1:3)\n\ngt_tbl &lt;- gt(m1_table)\n\ngt_tbl &lt;- gt_tbl |&gt; \n    tab_header(\n        title = \"Model 1 parameter and uncertainty interval estimates\",\n        subtitle = \"Using R with rstnarm\"\n) |&gt; \n    cols_label(\n        term = md(\"**parameter**\"),\n        estimate = md(\"**estimate**\"),\n        conf.low = md(\"**5% CI**\"),\n        conf.high = md(\"**95% CI**\"),\n        std.error = md(\"**std.error**\")\n    ) |&gt; \n    fmt_number(\n        decimals = 2\n    )\n\n\ngt_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1 parameter and uncertainty interval estimates\n\n\nUsing R with rstnarm\n\n\nparameter\nestimate\nstd.error\n5% CI\n95% CI\n\n\n\n\n(Intercept)\n0.87\n0.56\n−0.10\n1.87\n\n\npercentage.black\n0.11\n0.02\n0.08\n0.13\n\n\nsigma\n1.72\n0.24\n1.40\n2.19"
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#modelling-process-following-ros-chs-6-9",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#modelling-process-following-ros-chs-6-9",
    "title": "2: Simple linear regression",
    "section": "4 Modelling process (following ROS chs 6-9)",
    "text": "4 Modelling process (following ROS chs 6-9)\n\n\n\n\n\n\nROS Ch 6 summary\n\n\n\n\n\nAt a purely mathematical level, the methods described in this book have two purposes: prediction and comparison. We can use regression to predict an outcome variable, or more precisely the distribution of the outcome, given some set of inputs. And we can compare these predictions for different values of the inputs, to make simple comparisons between groups, or to estimate causal effects, a topic to which we shall return in Chapters 18–21. In this chapter we use our favoured technique of fake-data simulation to understand a simple regression model, use a real-data example of height and earnings to warn against unwarranted causal interpretations, and discuss the historical origins of regression as it relates to comparisons and statistical adjustment.\n\n\n\n\n\n\n\n\n\nROS Ch 7 summary\n\n\n\n\n\nAs discussed in Chapter 1, regression is fundamentally a technology for predicting an outcome y from inputs x 1, x 2, . . . . In this chapter we introduce regression in the simple (but not trivial) case of a linear model predicting a continuous y from a single continuous x, thus fitting the model yi = a + bx i + error to data (x i, yi ), i = 1, . . . , n. We demonstrate with an applied example that includes the steps of fitting the model, displaying the data and fitted line, and interpreting the fit. We then show how to check the fitting procedure using fake-data simulation, and the chapter concludes with an explanation of how linear regression includes simple comparison as a special case.\n\n\n\n\n\n\n\n\n\nRos Ch8 summary\n\n\n\n\n\nMost of this book is devoted to examples and tools for the practical use and understanding of regression models, starting with linear regression with a single predictor and moving to multiple predictors, nonlinear models, and applications in prediction and causal inference. In this chapter we lay out some of the mathematical structure of inference for regression models and some algebra to help understand estimation for linear regression. We also explain the rationale for the use of the Bayesian fitting routine stan_glm and its connection to classical linear regression. This chapter thus provides background and motivation for the mathematical and computational tools used in the rest of the book.\n\n\n\nChapter 7 sequence\n\nread in the data, having tidied first if necessary\n\nmade easy on this occasion as the data is in an R package and is tidy (one row per observation, one column per variable)\n\ndefine your model\n\nFor the first and simplest model (m1) assume there is a straight line relation between the outcome (age) and the predictor (nose markings), though with uncertainty (“error”) due to one or all of sampling error, measurement error and model error. Our model is therefore the straight line equation ‘y = a + bx’ plus error, where a is the intercept (where the line meets the y axis) and x is the slope. So, age = a + b*percentage.black + error\n\n\nconvert the model into R code\n\nThere are several inference machines in statistics, all available in R. We’ll use Bayesian inference via the user-friendly rstanarm package, but regularly cross-reference examples to classical inference implements with the lm function of R’s base package stats. With rstanarm our first line of code is m1 &lt;- stan_glm(age ~ percentage.black, data = lion_noses) see Code 3.4 for more explanation Note To make rstanarm produce same as lm the code is stan_glm(age ~ percentage.black, data= lion_noses, prior_intercept=NULL, prior=NULL, prior_aux=NULL, algorithm=\"optimizing\")\n\nrun your model and look at the results\n\neither print as in code 3.5 or use the modelsummary package\n\n\n\n\nShow the code\nmodelsummary(m1, gof_map = NA, statistic = 'conf.int', conf_level = 0.9, shape = term ~ model + statistic)\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \n(1)\n\n        \n              \n                 \n                Est.\n                5.0 %\n                95.0 %\n              \n        \n        \n        \n                \n                  (Intercept)     \n                  0.873\n                  -0.095\n                  1.874\n                \n                \n                  percentage.black\n                  0.107\n                  0.079 \n                  0.132\n                \n        \n      \n    \n\n\n\n\nparameters\n\n\nparameter\nestimate\nuncertainty\n\n\n\n\na\n0.872\n0.603\n\n\nb\n0.107\n0.016\n\n\n\\(\\sigma\\)\n1.713\n0.227\n\n\n\nThese can also be calculated “by hand” from the posterior simulations\n\n\nparameters and uncertainty from model matrix (use array?) of posterior simulations\nsims &lt;- as.matrix(m1)\nestimate_a &lt;- round(median(sims[,1]),3) \nestimate_b &lt;- round(median(sims[,2]),3)\nestimate_c &lt;- round(median(sims[,3]),3)\nci_a &lt;- round(quantile(sims[,1], c(0.025, 0.975),3))\nci_b &lt;- round(quantile(sims[,2], c(0.025, 0.975),3))\n\nprintL(\n\"the estimated intercept is\" = estimate_a,\n\"the estimated slope is\"=estimate_b,\n\"the estimated residual standard deviation is\"=estimate_c,\n\"the intercept cis are\"=ci_a,\n\"the slope cis are\"=ci_b\n)\n\n\nthe estimated intercept is: 0.873\n\nthe estimated slope is: 0.107\n\nthe estimated residual standard deviation is: 1.719\n\nthe intercept cis are:\n 2.5% 97.5% \n    0     2 \n\nthe slope cis are:\n 2.5% 97.5% \n    0     0 \n\n\nSo the result for \\(\\sigma\\) the residual standard deviation means that age will be within +/- 2*1.7 = +/-3.4 years 95% of the time.\nThe R2 is given by R2 &lt;- 1 - sigma(m1)^2 / sd(lion_noses$age)^2\n\n\nShow the code\nr2 &lt;- 1- sigma(m1)^2 / sd(lion_noses$age)^2\nprint(r2)\n\n\n[1] 0.5877698\n\n\nSo almost 60% of the variance in age can be explained by nose markings. Where is the other 40%? Probably in other variables (lion health?) the model did not include.\n\n\nCode 3.5 model 1 output\nprint(m1, digits = 3)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      age ~ percentage.black\n observations: 32\n predictors:   2\n------\n                 Median MAD_SD\n(Intercept)      0.873  0.563 \npercentage.black 0.107  0.015 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.719  0.233 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nCode 3.6 model 1 summary\nsummary(m1, digits = 3)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      age ~ percentage.black\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 32\n predictors:   2\n\nEstimates:\n                   mean   sd    10%   50%   90%\n(Intercept)      0.882  0.593 0.135 0.873 1.648\npercentage.black 0.106  0.016 0.086 0.107 0.126\nsigma            1.747  0.243 1.464 1.719 2.066\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 4.309  0.438 3.761 4.304 4.853\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse  Rhat  n_eff\n(Intercept)      0.010 1.001 3305 \npercentage.black 0.000 1.001 3450 \nsigma            0.005 1.000 2751 \nmean_PPD         0.007 1.000 3619 \nlog-posterior    0.033 1.001 1569 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nWhen the model m1 is run, a large amount of information is saved into m1, now visible in the R Studio environment. The essentials for interpreting the model are shown above, with the print(m1) command, but we could also look at any of the 27 stored objects individually, most of which we will not use, for example the coefficients (lines 8-9 above, hover over the code output) with m1$coefficients, which gives the same information as the Median column in the summary, although to more decimal places. This option to look more carefully at the output, rather than be presented with just a summary, is distinctive to R and other coding softwares.\nThe output is telling us (lines 7-9 highlighted), for now ignoring statistical uncertainty, that the best fit to the proposed model y = intercept + slope*percentage.black + error is:\n\nage = 0.9 + 0.1*percentage.black\n\nLet’s plot this line and the data.\n\n\nCode 3.6 A plot of the simple linear regression for model 1\np1 &lt;- ggplot(lion_noses, aes(percentage.black, age)) +\n    geom_point() +\n    geom_abline(intercept = 0.9, slope = 0.1) +\n    theme_bw() +\n    xlab(\"percentage black on nose\") +\n    ylab(\"age (years)\")\n\ndf &lt;- data.frame(\n  x = 26,\n  y = 5,\n  label = \"y = 0.9 + 0.1x\"\n)\n\np1 + geom_label(data = df, aes(x,y, label = label),\n                 color = \"black\", size = 5) +\n    xlim(0, 80)\n\n\n\n\n\n\n\n\n\nLet’s plug some numbers into the equation. Three lions L1, L2 and L3 are photographed in the field and subsequent investigation of the image estimates 42%, 50% and 60% nose markings. Using R as a pocket calculator, L1’s estimated age is given by the R code 0.9 + 0.1*42 = 5.1 years, L2’s 0.9 + 0.1*50 = 5.9 years and L3’s 0.9 + 0.1*60 = 6.9 years. As our model is a monotonic line (no curves), change in y as determined by the slope is constant and the general result is that;\nA 10% increase in nose markings equates to a 1 year increase in age.\nA safe interpretation is that, under model assumptions, the average difference in age, comparing two lions 10% different in nose markings, is one year. The safest interpretation of a regression is as a comparison. a nod towards effects and causal inference.\nTo summarize: regression is a mathematical tool for making predictions. Regression coefficients can sometimes be interpreted as effects, but they can always be interpreted as average comparisons.\nIf our model is correct, which we shall need to check, it is a better method of selecting which lions to hunt for trophies than the ad-hoc model, which assumed a lion with 40% nose markings was under 5 years. L1 met that threshold, but only just. If our measurer had been in a hurry and making small mistakes, whereas the actual measure was 40%, L1’s age should have been modelled as 0.9 +0.1*40 = 4.9 years and he could have not been made available to trophy hunters. CHECK THIS\nThe standard error for the slope (MAD_SD on line 9 of the model output Code 3.5) is 0.015, so the model has a slope of, to two decimal places, 0.11 +/- 0.01 and the 95% interval\n\n\nCode 3.7 Print the Confidence Intervals for model 1\nround(posterior_interval(m1, prob = 0.9), 2)\n\n\n                    5%  95%\n(Intercept)      -0.10 1.87\npercentage.black  0.08 0.13\nsigma             1.40 2.19\n\n\n\n\n\n\n\n\nROS Ch 9\n\n\n\n\n\nBayesian inference involves three steps that go beyond classical estimation. First, the data and model are combined to form a posterior distribution, which we typically summarize by a set of simulations of the parameters in the model. Second, we can propagate uncertainty in this distribution—that is, we can get simulation-based predictions for unobserved or future outcomes that accounts for uncertainty in the model parameters. Third, we can include additional information into the model using a prior distribution. The present chapter describes all three of these steps in the context of examples capturing challenges of prediction and inference.\n\n\n\nFor now we’ll not worry about priors and use defaults as per Bayes Rules!, complicated and we have no prior information to feed into the model, but concentrate on the posterior simulations to get estimates.\nAfter fitting a regression, y = a + bx + error, we can use it to predict a new data point, or a set of new data points, with predictors x new . We can make three sorts of predictions, corresponding to increasing levels of uncertainty:\n\nThe point prediction, \\(\\hat{a}\\) + \\(\\hat{b}\\)x new : Based on the fitted model, this is the best point estimate of the average value of y for new data points with this new value of x. We use \\(\\hat{a}\\) and \\(\\hat{b}\\) here because the point prediction ignores uncertainty.\nThe linear predictor with uncertainty, a + bx new , propagating the inferential uncertainty in (a, b). This represents the distribution of uncertainty about the expected or average value of y for new data points with predictors x new .\nThe predictive distribution for a new observation, a + bx new + error: This represents uncertainty about a new observation y with predictors x new .\n\nConsider our study, with y (age) predicted from x (nose markings).\n\n\nThe linear predictor is the modeled average age of lions with nose marking x new in the population, with uncertainty corresponding to inferential uncertainty in the coefficients a and b.\nThe predictive distribution represents the age of a single lion drawn at random from this population, under the model conditional on the specified value of x new .\n\nAs sample size approaches infinity, the coefficients a and b are estimated more and more precisely, and the uncertainty in the linear predictor approaches zero, but the uncertainty in the predictive distribution for a new observation does not approach zero; it approaches the residual standard deviation \\(\\sigma\\) (sigma in Table 1).\nConsider a lion on territory, so it can be re-found for trophy hunting, which is photographed and the nose markings measured as 45%. Let’s calculate the three measures for this individual.\n\nPoint estimate\nThe point estimate where the best fit regression line can be approximately read off the graph Code 3.6 as the value of y where x = 45. It is calculated with the base r function predict\n\n\nShow the code\nnew &lt;- data.frame(percentage.black=45)\ny_point_pred &lt;- round(predict(m1, newdata=new), 1)\ncat(\"The point prediction of the average age in years of a lion with \\n45% nose markings is\", y_point_pred)\n\n\nThe point prediction of the average age in years of a lion with \n45% nose markings is 5.7\n\n\n\n\nLinear predictor\nTo put uncertainty intervals around the point estimate, the rstanarm function posterior_linpred is used\n\n\nShow the code\ny_linpred &lt;- round(posterior_linpred(m1, newdata=new), 2)\nci_b &lt;- quantile(y_linpred, c(0.05, 0.95))\ncat(\"The linear predictor of the average age in years \\nof a lion with 45% nose markings are given by the \\n90% uncertainty intervals\", ci_b)\n\n\nThe linear predictor of the average age in years \nof a lion with 45% nose markings are given by the \n90% uncertainty intervals 5.05 6.27\n\n\n\n\nPredictive uncertainty\nThis uses the rstanarm function posterior_predict\n\n\nShow the code\ny_pred &lt;- posterior_predict(m1, newdata=new)\nMedian &lt;- median(y_pred)\nMAD_SD &lt;- mad(y_pred)\nyoung &lt;- mean(y_pred &lt; 5)\nuncertainty_90 &lt;- quantile(y_pred, c(0.05, 0.95))\nprintL(\"The predicted age of a lion with 45% nose markings is\"=round(Median,1),\n  \"with uncertainty intervals\"=round(uncertainty_90, 1),\n  \"The probability that a lion with 45% nose marking is under \\n5 years' age is\" =round(young, 2)\n  )\n\n\nThe predicted age of a lion with 45% nose markings is: 5.6\n\nwith uncertainty intervals:\n 5% 95% \n2.9 8.7 \n\nThe probability that a lion with 45% nose marking is under \n5 years' age is: 0.35\n\n\nIt is also possible to calculate any of these three over a range of values. Let’s do that for the range of nose markings for which we have data (a rule of thumb to not predict outside of the data range) so between 10-80% and at 5% intervals\n\n\nShow the code\nnew_grid &lt;- data.frame(percentage.black=seq(10, 80, 5))\ny_point_pred_grid &lt;- predict(m1, newdata=new_grid)# 15 values\ny_point_pred_df &lt;- as.data.frame(y_point_pred_grid)\ny_linpred_grid &lt;- posterior_linpred(m1, newdata=new_grid) # 4000 sims for each of 15 values\ny_linpred_cis &lt;- as.data.frame(y_linpred_grid) \ny_pred_grid &lt;- posterior_predict(m1, newdata=new_grid) # 4000 sims for each of 15 values\ny_pred_cis &lt;- as.data.frame(y_pred_grid)\n\nquants &lt;- c(0.05,0.95)\nlinear_uncertainty &lt;- apply( y_linpred_cis[1:15] , 2 , quantile , probs = quants , na.rm = TRUE )\nprediction_uncertainty &lt;- apply( y_pred_cis[1:15] , 2 , quantile , probs = quants , na.rm = TRUE )\n\nlinear_uncertainty_t &lt;- t(linear_uncertainty)\nprediction_uncertainty_t &lt;- t(prediction_uncertainty)\n\nintervals &lt;- as.data.frame(seq(10, 80, 5))\n\nm1_estimates &lt;- bind_cols( intervals, y_point_pred_df, linear_uncertainty_t, prediction_uncertainty_t)\n\n\nNew names:\n• `5%` -&gt; `5%...3`\n• `95%` -&gt; `95%...4`\n• `5%` -&gt; `5%...5`\n• `95%` -&gt; `95%...6`\n\n\nShow the code\n# to complete\n\ngt_tbl &lt;- gt(m1_estimates)\ngt_tbl |&gt;\n    tab_header(\n        title = \"Estimates of point, point uncertainty and prediction uncertainty for lion age from nose markings\",\n        subtitle = \"Estimates at 5% intervals for nose markings over the range of the sample data\"\n    ) |&gt; \n    fmt_number(\n        decimals = 1\n    ) |&gt; \n    cols_label(\n    \"seq(10, 80, 5)\" = \"Nose markings (percent)\",\n    y_point_pred_grid = \"point prediction of mean age (years)\",\n    \"5%...3\" = \"5%\",\n    \"95%...4\" = \"95%\",\n    \"5%...5\" = \"5%\",\n    \"95%...6\" = \"95%\"\n  ) |&gt; \n    tab_spanner(\n    label = \"uncertainty intervals for mean age\",\n    columns = c(3:4)\n  ) |&gt; \n    tab_spanner(\n    label = \"uncertainty intervals for individual age prediction\",\n    columns = c(5:6)\n  ) |&gt; \n    cols_align(\n    align = \"center\",\n    columns = c(1:6)\n  )\n\n\n\n\n\n\n\n\nEstimates of point, point uncertainty and prediction uncertainty for lion age from nose markings\n\n\nEstimates at 5% intervals for nose markings over the range of the sample data\n\n\nNose markings (percent)\npoint prediction of mean age (years)\nuncertainty intervals for mean age\nuncertainty intervals for individual age prediction\n\n\n5%\n95%\n5%\n95%\n\n\n\n\n10.0\n1.9\n1.2\n2.7\n−1.0\n4.9\n\n\n15.0\n2.5\n1.8\n3.2\n−0.5\n5.4\n\n\n20.0\n3.0\n2.4\n3.6\n0.1\n6.0\n\n\n25.0\n3.5\n3.0\n4.1\n0.7\n6.5\n\n\n30.0\n4.1\n3.6\n4.6\n1.1\n7.0\n\n\n35.0\n4.6\n4.1\n5.1\n1.7\n7.5\n\n\n40.0\n5.1\n4.6\n5.7\n2.2\n8.1\n\n\n45.0\n5.7\n5.0\n6.3\n2.7\n8.7\n\n\n50.0\n6.2\n5.5\n6.9\n3.0\n9.3\n\n\n55.0\n6.7\n5.9\n7.5\n3.9\n9.7\n\n\n60.0\n7.3\n6.4\n8.1\n4.1\n10.3\n\n\n65.0\n7.8\n6.8\n8.8\n4.8\n10.8\n\n\n70.0\n8.3\n7.2\n9.4\n5.2\n11.4\n\n\n75.0\n8.9\n7.6\n10.1\n5.7\n12.0\n\n\n80.0\n9.4\n8.0\n10.7\n6.2\n12.5\n\n\n\n\n\n\n\n\n\nestimates from simulation matrix\nsims &lt;- as.matrix(m1)\nMedian &lt;- round(apply(sims, 2, median),3)\nMAD_SD &lt;- round(apply(sims, 2, mad),3)\nprint(cbind(Median, MAD_SD))\n\n\n                 Median MAD_SD\n(Intercept)       0.873  0.563\npercentage.black  0.107  0.015\nsigma             1.719  0.233\n\n\n\n\nCode 3.9 A comparison of classical and Bayesian models\nmodels &lt;- list(\n  \"OLS classical\" = lm(age ~ percentage.black, data = lion_noses),\n  \"OLS rstanarm default priors\" = stan_glm(age ~ percentage.black, data = lion_noses),\n  \"OLS rstnarm flat priors\" = stan_glm(age ~ percentage.black, data = lion_noses, prior_intercept = NULL, prior = NULL, prior_aux = NULL)\n)\n\n\n\n\nCode 3.10 A comparison of classical and Bayesian models\nmodelsummary(models, gof_map = NA, statistic = 'conf.int', conf_level = 0.95, stars = TRUE, shape = term ~ model + statistic)\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nOLS classical\nOLS rstanarm default priors\nOLS rstnarm flat priors\n\n        \n              \n                 \n                Est.\n                2.5 %\n                97.5 %\n                Est.\n                2.5 %\n                97.5 %\n                Est.\n                2.5 %\n                97.5 %\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)     \n                  0.879   \n                  -0.283\n                  2.041\n                  0.884\n                  -0.319\n                  2.119\n                  0.878\n                  -0.350\n                  2.088\n                \n                \n                  percentage.black\n                  0.106***\n                  0.076 \n                  0.137\n                  0.106\n                  0.074 \n                  0.138\n                  0.107\n                  0.076 \n                  0.139\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\nCode 3.11 Plot mean uncertainty\nlion_noses %&gt;%\n    tidybayes::add_epred_draws(m1) %&gt;%\n    ggplot(aes(x = percentage.black, y = age)) +\n    stat_lineribbon(aes(y = .epred), .width = c(0.5, 0.9), color = \"#08519C\") +\n    geom_point(data = lion_noses) +\n    scale_fill_brewer()\n\n\n\n\n\n\n\n\n\n\n\nCode 3.12 Plot individual uncertainty\nlion_noses %&gt;%\n    tidybayes::add_predicted_draws(m1) %&gt;%\n    ggplot(aes(x = percentage.black, y = age)) +\n    stat_lineribbon(aes(y = .prediction), .width = c(0.5, 0.9), color = \"#08519C\") +\n    geom_point(data = lion_noses) +\n    theme_bw() +\n    scale_fill_brewer()\n\n\n\n\n\n\n\n\n\n\n\nCode 3.13 Plot mean and individual uncertainty combined\np1 &lt;- lion_noses |&gt; \n    tidybayes::add_epred_draws(m1) |&gt; \n    ggplot(aes(x = percentage.black, y = age)) +\n    stat_lineribbon(aes(y = .epred), .width = c(0.5, 0.9), color = \"#08519C\") +\n    geom_point(data = lion_noses) +\n    scale_fill_brewer()\np2 &lt;- lion_noses |&gt;     \n    tidybayes::add_predicted_draws(m1) |&gt; \n    ggplot(aes(x = percentage.black, y = age)) +\n    stat_lineribbon(aes(y = .prediction), .width = c(0.5, 0.9), color = \"#08519C\") +\n    geom_point(data = lion_noses) +\n    theme_bw() +\n    scale_fill_brewer()\n\np3 &lt;- lion_noses |&gt; \n    tidybayes::add_epred_draws(m1)\n\np4 &lt;- lion_noses |&gt; \n    tidybayes::add_predicted_draws(m1)\n\np5 &lt;- bind_rows(p3, p4)\n\np5 |&gt; \nggplot(aes(x = percentage.black, y = age)) +\n    stat_lineribbon(aes(y = .epred), .width = c(0.95), color = \"#08519C\") +\n    stat_lineribbon(aes(y = .prediction), .width = c(0.9), color = \"#08519C\", alpha = 0.2) +\n    geom_point(data = lion_noses) +\n    theme_bw() +\n    scale_fill_brewer() +\n    guides(fill = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode 3.14 Plot mean and individual uncertainty side by side\np1 / p2"
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#bayes-rules-modelling-sequence-from-chapter-9",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#bayes-rules-modelling-sequence-from-chapter-9",
    "title": "2: Simple linear regression",
    "section": "5 Bayes Rules modelling sequence; from Chapter 9",
    "text": "5 Bayes Rules modelling sequence; from Chapter 9\n\n5.1 specify the data model\nequation. Maths is shorthand, translate into english\n\\(Y_i|\\beta_0,\\beta_1,\\sigma\\) \\(\\simeq\\) \\(N(\\mu_1, \\sigma^2)\\) with \\(\\mu_1 = \\beta_0 +\\beta_1X_1\\)\n\ncoefficients and parameters alternative names.\nadd fig 9.2 (simulate) to show how same parameters for slope and intercept ca arise with different sigmas.\n\n\n\n5.2 Specify the priors\nExplain why defaults in rstnarm are ok\n\n\n5.3 Posterior simulation\n\n\nShow the code\nbayesplot::neff_ratio(m1)\n\n\n     (Intercept) percentage.black            sigma \n         0.82625          0.86250          0.68775 \n\n\n\n\nShow the code\nbayesplot::rhat(m1)\n\n\n     (Intercept) percentage.black            sigma \n        1.000691         1.001134         1.000174 \n\n\n\n\nShow the code\nbayesplot::mcmc_trace(m1, size = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nbayesplot::mcmc_dens_overlay(m1)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlion_noses %&gt;%\n  add_fitted_draws(m1, n = 100) %&gt;% \n  ggplot(aes(x = percentage.black, y = age)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + \n    geom_point(data = lion_noses, size = 0.05)\n\n\nWarning in fitted_draws.default(model = model, newdata = newdata, ..., n = n): `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`."
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#present-conclusions",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#present-conclusions",
    "title": "2: Simple linear regression",
    "section": "6 Present conclusions",
    "text": "6 Present conclusions\nyaba yaba"
  },
  {
    "objectID": "statsposts/2024-08-19-simple-linear-regression/index.html#interpret-in-subject-matter-terms",
    "href": "statsposts/2024-08-19-simple-linear-regression/index.html#interpret-in-subject-matter-terms",
    "title": "2: Simple linear regression",
    "section": "7 Interpret in subject matter terms",
    "text": "7 Interpret in subject matter terms\n\nShiny app\nIt is often useful in communicating your results to have an interactive app that can, in this case, allow someone to enter a nose marking measure and get output on the lion’s age with the model hidden. This has become increasingly easy in R with Shiny. We’ll follow the official Shiny website and also a nice example on spatio-temporal data from (Morega 2020) which is free online\n\n\n7.1 Notes on stuff to add/added\nadded quarto line-highlight extension for highlighting code, including output code and fontawesome for icons\nfinal section shiny app??\nCheck Paula Morega\nadd labels to code chunks cv quarto template\nadd targets\nsam csik’s blog article has instructions and code, how to add footnotes and a bibliography and citations and how to populate margins. Use these. also recommends W3 schools for css/scss/html\n\n\n7.2 refs and open books to add\nR for Data Science 2e and source code on GitHub\nTidy Modeling with R\nR Markdown Cookbook online\nggplot2 3e online\nR Graphics Cookbook online ggplot2 based\nBayes Rules! An Introduction to Applied Bayesian Modelling uses rstanarm\nFrank Harrell’s R workflow, inlcuding Quarto\nHarrell’s associated course\nhuxtable for regression tables, Andrew Heiss’ blog\nHuxtable R table package recommended for regression tables\npatchwork r package to reorganise image layout\nhtml validator\nLaTex symbols\nefficient R programming\nmodern dive"
  }
]